{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image size for our model.\n",
    "MODEL_INPUT_IMAGE_SIZE = [ 200 , 200 ]\n",
    "\n",
    "# Fraction of the dataset to be used for testing.\n",
    "TRAIN_TEST_SPLIT = 0.3\n",
    "\n",
    "# Number of samples to take from dataset\n",
    "N = 20000\n",
    "\n",
    "# This method will be mapped for each filename in `list_ds`. \n",
    "def parse_image( filename ):\n",
    "\n",
    "    # Read the image from the filename and resize it.\n",
    "    image_raw = tf.io.read_file( filename )\n",
    "    image = tf.image.decode_jpeg( image_raw , channels=3 ) \n",
    "    image = tf.image.resize( image , MODEL_INPUT_IMAGE_SIZE ) / 255\n",
    "\n",
    "    # Split the filename to get the age and the gender. Convert the age ( str ) and the gender ( str ) to dtype float32.\n",
    "    parts = tf.strings.split( tf.strings.split( filename , '/' )[ 2 ] , '_' )\n",
    "\n",
    "    # Normalize\n",
    "    age = tf.strings.to_number( parts[ 0 ] ) / 116\n",
    "\n",
    "    return image , age\n",
    "\n",
    "# List all the image files in the given directory.\n",
    "list_ds = tf.data.Dataset.list_files( 'data/utkface23k/*',shuffle=True )\n",
    "\n",
    "# Map `parse_image` method to all filenames.\n",
    "dataset = list_ds.map( parse_image , num_parallel_calls=tf.data.AUTOTUNE )\n",
    "dataset = dataset.take( N )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\nithi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/nithi/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Create train and test splits of the dataset.\n",
    "num_examples_in_test_ds = int( dataset.cardinality().numpy() * TRAIN_TEST_SPLIT )\n",
    "\n",
    "test_ds = dataset.take( num_examples_in_test_ds )\n",
    "train_ds = dataset.skip( num_examples_in_test_ds )\n",
    "\n",
    "print( 'Num examples in train ds {}'.format( train_ds.cardinality() ) )\n",
    "print( 'Num examples in test ds {}'.format( test_ds.cardinality() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\nithi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/nithi/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Define the conv block.\n",
    "if lite_model:\n",
    "        x = tf.keras.layers.SeparableConv2D( num_filters ,\n",
    "                                            kernel_size=kernel_size ,\n",
    "                                            strides=strides \n",
    "                                            , use_bias=False ,\n",
    "                                            kernel_initializer=tf.keras.initializers.HeNormal() ,\n",
    "                                            kernel_regularizer=tf.keras.regularizers.L2( 1e-5 )\n",
    "                                             )( x )\n",
    "    else:\n",
    "        x = tf.keras.layers.Conv2D( num_filters ,\n",
    "                                   kernel_size=kernel_size ,\n",
    "                                   strides=strides ,\n",
    "                                   use_bias=False ,\n",
    "                                   kernel_initializer=tf.keras.initializers.HeNormal() ,\n",
    "                                   kernel_regularizer=tf.keras.regularizers.L2( 1e-5 )\n",
    "                                    )( x )\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()( x )\n",
    "    x = tf.keras.layers.LeakyReLU( leaky_relu_alpha )( x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\nithi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/nithi/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "ef dense( x , filters , dropout_rate ):\n",
    "    x = tf.keras.layers.Dense( filters , kernel_regularizer=tf.keras.regularizers.L2( 0.1 ) , bias_regularizer=tf.keras.regularizers.L2( 0.1 ) )( x )\n",
    "    x = tf.keras.layers.LeakyReLU( alpha=leaky_relu_alpha )( x )\n",
    "    x = tf.keras.layers.Dropout( dropout_rate )( x )\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\nithi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/nithi/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Negative slope coefficient for LeakyReLU.\n",
    "leaky_relu_alpha = 0.2\n",
    "\n",
    "lite_model = False\n",
    "\n",
    "# Define the conv block.\n",
    "def conv( x , num_filters , kernel_size=( 3 , 3 ) , strides=1 ):\n",
    "    if lite_model:\n",
    "        x = tf.keras.layers.SeparableConv2D( num_filters ,\n",
    "                                            kernel_size=kernel_size ,\n",
    "                                            strides=strides, \n",
    "                                            use_bias=False ,\n",
    "                                            kernel_initializer=tf.keras.initializers.HeNormal() ,\n",
    "                                            kernel_regularizer=tf.keras.regularizers.L2( 1e-5 )\n",
    "                                             )( x )\n",
    "    else:\n",
    "        x = tf.keras.layers.Conv2D( num_filters ,\n",
    "                                   kernel_size=kernel_size ,\n",
    "                                   strides=strides ,\n",
    "                                   use_bias=False ,\n",
    "                                   kernel_initializer=tf.keras.initializers.HeNormal() ,\n",
    "                                   kernel_regularizer=tf.keras.regularizers.L2( 1e-5 )\n",
    "                                    )( x )\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()( x )\n",
    "    x = tf.keras.layers.LeakyReLU( leaky_relu_alpha )( x )\n",
    "    return x\n",
    "\n",
    "def dense( x , filters , dropout_rate ):\n",
    "    x = tf.keras.layers.Dense( filters , kernel_regularizer=tf.keras.regularizers.L2( 0.1 ) , bias_regularizer=tf.keras.regularizers.L2( 0.1 ) )( x )\n",
    "    x = tf.keras.layers.LeakyReLU( alpha=leaky_relu_alpha )( x )\n",
    "    x = tf.keras.layers.Dropout( dropout_rate )( x )\n",
    "    return x\n",
    "\n",
    "\n",
    "# No. of convolution layers to be added.\n",
    "num_blocks = 6\n",
    "# Num filters for each conv layer.\n",
    "num_filters = [ 16 , 32 , 64 , 128 , 256 , 256 ]\n",
    "# Kernel sizes for each conv layer.\n",
    "kernel_sizes = [ 3 , 3 , 3 , 3 , 3 , 3 ]\n",
    "\n",
    "# Init a Input Layer.\n",
    "inputs = tf.keras.layers.Input( shape=MODEL_INPUT_IMAGE_SIZE + [ 3 ] )\n",
    "\n",
    "# Add conv blocks sequentially\n",
    "x = inputs\n",
    "for i in range( num_blocks ):\n",
    "    x = conv( x , num_filters=num_filters[ i ] , kernel_size=kernel_sizes[ i ] )\n",
    "    x = tf.keras.layers.MaxPooling2D()( x )\n",
    "\n",
    "# Flatten the output of the last Conv layer.\n",
    "    x = tf.keras.layers.Flatten()( x )\n",
    "conv_output = x \n",
    "\n",
    "# Add Dense layers ( Dense -> LeakyReLU -> Dropout )\n",
    "x = dense( conv_output , 256 , 0.6 )\n",
    "x = dense( x , 64 , 0.4 )\n",
    "x = dense( x , 32 , 0.2 )\n",
    "outputs = tf.keras.layers.Dense( 1 , activation='relu' )( x )\n",
    "\n",
    "# Build the Model\n",
    "model = tf.keras.models.Model( inputs , outputs )\n",
    "\n",
    "# Uncomment the below to view the summary of the model.\n",
    "model.summary()\n",
    "# tf.keras.utils.plot_model( model , to_file='architecture.png' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\nithi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/nithi/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def scheduler( epochs , learning_rate ):\n",
    "    if epochs < num_epochs * 0.25:\n",
    "        return learning_rate\n",
    "    elif epochs < num_epochs * 0.5:\n",
    "        return 0.0005\n",
    "    elif epochs < num_epochs * 0.75:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.000095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_epochs = 50 #@param {type: \"number\"}\n",
    "batch_size = 128 #@param {type: \"number\"}\n",
    "\n",
    "# Batch and repeat `train_ds` and `test_ds`.\n",
    "train_ds = train_ds.batch( batch_size )\n",
    "test_ds = test_ds.batch( batch_size )\n",
    "\n",
    "# Init ModelCheckpoint callback\n",
    "save_dir_ = 'model_1'  #@param {type: \"string\"}\n",
    "save_dir = save_dir_ + '/{epoch:02d}-{val_mae:.2f}.h5'\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint( \n",
    "    save_dir , \n",
    "    save_best_only=True , \n",
    "    monitor='val_mae' , \n",
    "    mode='min' , \n",
    ")\n",
    "\n",
    "tb_log_name = 'model_1'  #@param {type: \"string\"}\n",
    "# Init TensorBoard Callback\n",
    "logdir = os.path.join( \"tb_logs\" , tb_log_name )\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard( logdir )\n",
    "# Init LR Scheduler\n",
    "def scheduler( epochs , learning_rate ):\n",
    "    if epochs < num_epochs * 0.25:\n",
    "        return learning_rate\n",
    "    elif epochs < num_epochs * 0.5:\n",
    "        return 0.0005\n",
    "    elif epochs < num_epochs * 0.75:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.000095\n",
    "\n",
    "lr_schedule_callback = tf.keras.callbacks.LearningRateScheduler( scheduler )\n",
    "\n",
    "# Init Early Stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping( monitor='val_mae' , patience=10 )\n",
    "\n",
    "# Compile the model\n",
    "model.compile( \n",
    "    loss=tf.keras.losses.mean_absolute_error ,\n",
    "    optimizer = tf.keras.optimizers.Adam( learning_rate ) , \n",
    "    metrics=[ 'mae' ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative slope coefficient for LeakyReLU.\n",
    "leaky_relu_alpha = 0.2\n",
    "\n",
    "lite_model = False\n",
    "\n",
    "# Define the conv block.\n",
    "def conv( x , num_filters , kernel_size=( 3 , 3 ) , strides=1 ):\n",
    "    if lite_model:\n",
    "        x = tf.keras.layers.SeparableConv2D( num_filters ,\n",
    "                                            kernel_size=kernel_size ,\n",
    "                                            strides=strides, \n",
    "                                            use_bias=False ,\n",
    "                                            kernel_initializer=tf.keras.initializers.HeNormal() ,\n",
    "                                            kernel_regularizer=tf.keras.regularizers.L2( 1e-5 )\n",
    "                                             )( x )\n",
    "    else:\n",
    "        x = tf.keras.layers.Conv2D( num_filters ,\n",
    "                                   kernel_size=kernel_size ,\n",
    "                                   strides=strides ,\n",
    "                                   use_bias=False ,\n",
    "                                   kernel_initializer=tf.keras.initializers.HeNormal() ,\n",
    "                                   kernel_regularizer=tf.keras.regularizers.L2( 1e-5 )\n",
    "                                    )( x )\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()( x )\n",
    "    x = tf.keras.layers.LeakyReLU( leaky_relu_alpha )( x )\n",
    "    return x\n",
    "def dense( x , filters , dropout_rate ):\n",
    "    x = tf.keras.layers.Dense( filters , kernel_regularizer=tf.keras.regularizers.L2( 0.1 ) , bias_regularizer=tf.keras.regularizers.L2( 0.1 ) )( x )\n",
    "    x = tf.keras.layers.LeakyReLU( alpha=leaky_relu_alpha )( x )\n",
    "    x = tf.keras.layers.Dropout( dropout_rate )( x )\n",
    "    return x\n",
    "\n",
    "\n",
    "# No. of convolution layers to be added.\n",
    "num_blocks = 6\n",
    "# Num filters for each conv layer.\n",
    "num_filters = [ 16 , 32 , 64 , 128 , 256 , 256 ]\n",
    "# Kernel sizes for each conv layer.\n",
    "kernel_sizes = [ 3 , 3 , 3 , 3 , 3 , 3 ]\n",
    "\n",
    "# Init a Input Layer.\n",
    "inputs = tf.keras.layers.Input( shape=MODEL_INPUT_IMAGE_SIZE + [ 3 ] )\n",
    "\n",
    "# Add conv blocks sequentially\n",
    "x = inputs\n",
    "for i in range( num_blocks ):\n",
    "    x = conv( x , num_filters=num_filters[ i ] , kernel_size=kernel_sizes[ i ] )\n",
    "    x = tf.keras.layers.MaxPooling2D()( x )\n",
    "\n",
    "# Flatten the output of the last Conv layer.\n",
    "x = tf.keras.layers.Flatten()( x )\n",
    "conv_output = x \n",
    "\n",
    "# Add Dense layers ( Dense -> LeakyReLU -> Dropout )\n",
    "x = dense( conv_output , 256 , 0.6 )\n",
    "x = dense( x , 64 , 0.4 )\n",
    "x = dense( x , 32 , 0.2 )\n",
    "outputs = tf.keras.layers.Dense( 1 , activation='relu' )( x )\n",
    "\n",
    "# Build the Model\n",
    "model = tf.keras.models.Model( inputs , outputs )\n",
    "\n",
    "# Uncomment the below to view the summary of the model.\n",
    "model.summary()\n",
    "# tf.keras.utils.plot_model( model , to_file='architecture.png' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\nithi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/nithi/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit( \n",
    "    train_ds, \n",
    "    epochs=num_epochs,  \n",
    "    validation_data=test_ds, \n",
    "    callbacks=[ checkpoint_callback , tensorboard_callback , lr_schedule_callback , early_stopping_callback ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
